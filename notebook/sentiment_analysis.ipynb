{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "euWcIvAV2LVz",
        "yeCxeey-2SRh",
        "vajP5k8v2XUC",
        "z3zxWjFV-Ftz",
        "7n_bNUcaCe3Y",
        "sCzzHaYU5Yoo",
        "TF_VcpXriOcL",
        "eNqAWVzgmWS9",
        "CLelRPYWnCCB",
        "vWbOiyCKpheZ",
        "5L3XIx7YsCx9",
        "_rMQ2R0Y5Mnf",
        "KfwldBu1gWLG",
        "N2nAlnK2hz60",
        "x0OoK9OTphrT",
        "M6Kpc91_phzi",
        "yMynh2Rgo5vL",
        "XiiSQAKrpJga",
        "k3Ln5Z6laG1T"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéà Belajar Python Dasar"
      ],
      "metadata": {
        "id": "K1sGfcKY0vL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Belajar Python sangat menyenangkan.\n",
        "\n",
        "Gek Ana : [Colab Gek Ana](https://colab.research.google.com/drive/1Upid3MR8aXfZWk9y-Ow5nMYDJIkTfu3o#scrollTo=VH5_3ZLB4omQ)\n",
        "\n",
        "Gek Ani : [Colab Gek Ani](https://colab.research.google.com/drive/1DZ8vIzTFJ77Itdi6OUXaOr0mRDC8MMXF?usp=sharing#scrollTo=NWAUBmQB2UJk)\n"
      ],
      "metadata": {
        "id": "g0j3Lpjd4wJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üêç Python Basic"
      ],
      "metadata": {
        "id": "euWcIvAV2LVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1Ô∏è‚É£ Numbers"
      ],
      "metadata": {
        "id": "yeCxeey-2SRh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAb6LT62uoYy"
      },
      "outputs": [],
      "source": [
        "tinggi = 12\n",
        "alas = 20\n",
        "\n",
        "luas = (tinggi * alas)/2\n",
        "\n",
        "print(\"luas segitiga adalah\", int(luas))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2Ô∏è‚É£ **Text**"
      ],
      "metadata": {
        "id": "vajP5k8v2XUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nama=\"Ngakan Putu Widyasprana\"\n",
        "print(nama[0:6])\n",
        "print(nama[7:12] + nama[0:6])\n",
        "\n",
        "print(\"Panjang Text=\", len(nama))"
      ],
      "metadata": {
        "id": "SDnWe8cC2bF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3Ô∏è‚É£ **List**"
      ],
      "metadata": {
        "id": "z3zxWjFV-Ftz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "List is an array method in python that can be changeable and have ordered number in variabel."
      ],
      "metadata": {
        "id": "jv3qiQcZk1lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "identitas = ['Ngakan', 2, '19 November xxxx']\n",
        "identitas[:]\n",
        "identitas.append(\"Mangga Muda\")\n",
        "print(identitas)\n",
        "print(\"Isi list :\", len(identitas))"
      ],
      "metadata": {
        "id": "73uHHbYh-_ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nama_siswa = [\"Ngakan\", \"Gek Ani\", \"Gek Ana\"]\n",
        "umur_siswa = [17, 20, 25]\n",
        "\n",
        "data_siswa = [nama_siswa, umur_siswa]\n",
        "print(f\"Nama Siswa: {data_siswa[0][0]}, Umur : {data_siswa[1][0]}\")"
      ],
      "metadata": {
        "id": "o9Wug3uVETKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_siswa[2][0])"
      ],
      "metadata": {
        "id": "gPUbHsFfJp5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4Ô∏è‚É£ **Perulangan**"
      ],
      "metadata": {
        "id": "7n_bNUcaCe3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan While\n",
        "\n",
        "# Algortima Bilangan Fibonachi\n",
        "# Step 1: Definisi Variabel A0 dan An\n",
        "A0 = 0\n",
        "An = 1\n",
        "Bil_now = 0\n",
        "Max = 100\n",
        "\n",
        "# Step 2 : Looping Sejumlah Maximal\n",
        "while Bil_now < Max:\n",
        "\n",
        "    # Step 3: Cetak Angka Sekarang (Bil_now)\n",
        "    print(Bil_now)\n",
        "\n",
        "    # Step 4: Ubah nilai variabel A0 dan An\n",
        "    A0 = An\n",
        "    An = Bil_now\n",
        "\n",
        "    # Step 5 : Update nilai Bil_now\n",
        "    Bil_now = A0 + An"
      ],
      "metadata": {
        "id": "ONg1PzhbC_Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan For Loop\n",
        "\n",
        "maksimal = 20\n",
        "a, b = 0, 1\n",
        "\n",
        "for x in range(maksimal):\n",
        "    print(f\"Perulangan ke-{x} : {a}\")\n",
        "\n",
        "    # Menghitung nilai Fibonacci berikutnya\n",
        "    a, b = b, a + b\n"
      ],
      "metadata": {
        "id": "G88Txm-DGjBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Penggunaan For pada List\n",
        "\n",
        "identitas = ['Ngakan', 20, '19 November xxxx', 'SD Jambe Agung']\n",
        "\n",
        "# Mencari panjang list maximal\n",
        "isi_list = len(identitas)\n",
        "\n",
        "# for benda in range(3):\n",
        "#     print(f\"Perulangan : {identitas[benda]}\")\n",
        "\n",
        "for benda in identitas:\n",
        "    print(f\"Perulangan : {benda}\")"
      ],
      "metadata": {
        "id": "wP4H2FePJFAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nested Loop\n",
        "\n",
        "rows = 3\n",
        "columns = 3\n",
        "\n",
        "for i in range(rows):\n",
        "    for j in range(columns):\n",
        "        print(f\"({i}, {j})\", end=\" \")\n",
        "    print()  # Untuk pindah baris setelah kolom selesai"
      ],
      "metadata": {
        "id": "Jn2z5oBNMQV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nested Loop in Nested List (Matrix)\n",
        "\n",
        "matrix = [\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9]\n",
        "]"
      ],
      "metadata": {
        "id": "b6JgKDm0N-L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nested Loop dengan deklarasi variabel kolom dan baris\n",
        "\n",
        "rows = 3\n",
        "columns = 3\n",
        "\n",
        "for i in range(rows):\n",
        "    for j in range(columns):\n",
        "        print(f\"({i}, {j}) = {matrix[i][j]}\")\n",
        "    print()  # Untuk pindah baris setelah kolom selesai"
      ],
      "metadata": {
        "id": "Gusj4BakRQ4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nested Loop tanpa deklarasi variabel kolom dan baris\n",
        "\n",
        "for i in range(len(matrix)):\n",
        "    for j in range(len(matrix[i])):\n",
        "        print(f\"({i}, {j}) = {matrix[i][j]}\")\n",
        "    print()  # Untuk pindah baris setelah kolom selesai"
      ],
      "metadata": {
        "id": "kdEar-YtPa9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(len(matrix)):\n",
        "  print(matrix[x])"
      ],
      "metadata": {
        "id": "ylmWvJrGQzjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5Ô∏è‚É£ **Function**"
      ],
      "metadata": {
        "id": "sCzzHaYU5Yoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def luas_segitiga(alas, tinggi):\n",
        "  luas = (alas * tinggi) / 2\n",
        "  print(luas)\n",
        "\n",
        "luas_segitiga(10, 5)"
      ],
      "metadata": {
        "id": "gZYopO7E5ex4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def luas_segitiga(alas, tinggi):\n",
        "  luas = (alas * tinggi) / 2\n",
        "  return luas\n",
        "\n",
        "hasil = luas_segitiga(10, 5)\n",
        "print(hasil)"
      ],
      "metadata": {
        "id": "5vHJtnOC6C2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mencetak_huruf_vokal():\n",
        "  print(\"AIUEO\")\n",
        "\n",
        "mencetak_huruf_vokal()"
      ],
      "metadata": {
        "id": "DwpoL8io6Q9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mencetak_huruf_vokal():\n",
        "  return \"AIUEO\"\n",
        "\n",
        "huruf = mencetak_huruf_vokal()\n",
        "print(huruf)"
      ],
      "metadata": {
        "id": "jmCN3a0m6h7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pembagian(a=0,b=1):\n",
        "  bagi = a / b\n",
        "  print(bagi)\n",
        "\n",
        "hasil_pem = pembagian()\n",
        "print(hasil_pem)"
      ],
      "metadata": {
        "id": "y06VrsT46qgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6Ô∏è‚É£ **Tuple**"
      ],
      "metadata": {
        "id": "TF_VcpXriOcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A array method which can unchangable and also have ordered number in variabel. Also tuple can have duplicate members."
      ],
      "metadata": {
        "id": "o0SG4geDiYh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keranjang_buah = (\"apel\", \"jeruk\", \"mangga\")\n",
        "print(keranjang_buah[0])"
      ],
      "metadata": {
        "id": "0_YtyZHflYAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7Ô∏è‚É£ **Set**"
      ],
      "metadata": {
        "id": "eNqAWVzgmWS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A array method from phyton unordered, unchangeable, and unindexed, no duplicate members."
      ],
      "metadata": {
        "id": "27SC0f02mfdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kranjang_buah = {\"apel\", \"jeruk\", \"mangga\", \"apel\", \"jeruk\", \"mangga\"}\n",
        "print(kranjang_buah)"
      ],
      "metadata": {
        "id": "AfjbQNOWme4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8Ô∏è‚É£ **Dictionary**"
      ],
      "metadata": {
        "id": "CLelRPYWnCCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An array method from pyhton which has value and key pair. Ordered, changeable, no duplicates."
      ],
      "metadata": {
        "id": "Tbk5tZK7nGrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_mobil = {\"Jenis\" : \"SUV\", \"Tahun\" : 2021, \"Warna\" : \"Hitam\"}\n",
        "print(model_mobil[\"Warna\"])"
      ],
      "metadata": {
        "id": "c0jON3T2nhxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9Ô∏è‚É£ **Perkondisian**"
      ],
      "metadata": {
        "id": "vWbOiyCKpheZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sebuah metode melakukan pemilihan keputusan, bisa dibuat bercabang atau tidak."
      ],
      "metadata": {
        "id": "nplNX_32ppFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Angka = 10\n",
        "\n",
        "if Angka > 10:\n",
        "  print(\"Angka lebih besar dari 10\")\n",
        "elif Angka < 10:\n",
        "  print(\"Angka lebih kecil dari 10\")\n",
        "else:\n",
        "  print(\"Angka sama dengan 10\")"
      ],
      "metadata": {
        "id": "RRkbB0Zbp0D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîü **Try Except**"
      ],
      "metadata": {
        "id": "5L3XIx7YsCx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A method from python that can give the code how to check code is there something wrong."
      ],
      "metadata": {
        "id": "yd2zSXFnsMlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  print(fg)\n",
        "  ...\n",
        "  ...\n",
        "\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "mqEQpjExsL4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìú **Quiz Time**\n"
      ],
      "metadata": {
        "id": "_rMQ2R0Y5Mnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ö° **First Quiz**"
      ],
      "metadata": {
        "id": "KfwldBu1gWLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Quiz can be access in Kahoot! The material in this quiz is about Numbers, Texts, List, and Loop.**\n",
        "\n",
        "There is the link : [kahoot!](https://create.kahoot.it/share/quiz-basic-python/79e16990-cc2b-4778-853a-c1b22af9f0ab)"
      ],
      "metadata": {
        "id": "Z6CHMr_15byi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Question Number 1 : About Numbers\n",
        "\n",
        "def calculator(a=1, b=2, c=3, d=4):\n",
        "\n",
        "  calculation_A = a + b\n",
        "  calculation_B = c * d\n",
        "\n",
        "  if calculation_B != 0 :\n",
        "    calculation_C = calculation_A / calculation_B\n",
        "    return calculation_C\n",
        "  else :\n",
        "    print(\"Undifined.\")"
      ],
      "metadata": {
        "id": "GFDeTKLa5Z37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Qustion Number 2 : About Strings\n",
        "\n",
        "def ganti_huruf_vokal(text):\n",
        "  huruf_vokal = \"aiueoAIUEO\"    # Filter\n",
        "  if text == \"\":\n",
        "    return \"Text Tidak Boleh Kosong\"\n",
        "\n",
        "  for huruf in huruf_vokal:\n",
        "    text = text.replace(huruf, \"_\")\n",
        "  return text\n",
        "\n",
        "ganti_huruf_vokal(\"A I e o\")"
      ],
      "metadata": {
        "id": "khajX0lF9aoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Question Number 3 : Modification List\n",
        "\n",
        "def tambah_nilai(number):\n",
        "  for angka in range (len(number)):\n",
        "    number[angka]+= 2\n",
        "    # number[angka] = number[angka] + 2\n",
        "\n",
        "    # Penjelasan\n",
        "    # number[0] = 1, 1 = 1 + 2 -> number[0] = 3\n",
        "    # .....\n",
        "    # number[4] = 5, 5 = 5 + 2 -> number[4] = 7\n",
        "\n",
        "  return number"
      ],
      "metadata": {
        "id": "Phx-KkpY_5Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number = [1, 2, 3, 4, 5]\n",
        "# hasil = tambah_nilai(number)\n",
        "# print(hasil)\n",
        "\n",
        "tambah_nilai(number)"
      ],
      "metadata": {
        "id": "OHWOZ2qrGLaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Question Number 4 : Loop\n",
        "\n",
        "for angka in range(1, 50):\n",
        "  # print(angka, end=\" \")\n",
        "  if angka % 9 == 0:\n",
        "    print(angka)"
      ],
      "metadata": {
        "id": "r52Jz6wNCFAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Queztion Number 5 : Nested Loop\n",
        "\n",
        "for kolom in range (1, 5):\n",
        "  for baris in range (1, 5):\n",
        "    print(f\"{kolom * baris:3}\", end=\" \")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "yNUOZf_4FuA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù **Essay Time**"
      ],
      "metadata": {
        "id": "59tXadCbOhUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ö° **First Essay**"
      ],
      "metadata": {
        "id": "xbSYFNnVgcXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Study Case :\n",
        "'''\n",
        "Dalam sebuah rak makanan terdapat beberapa jenis makanan dan minuman yang memiliki brand dan informasi terkait dengan barangnya, berikut adalah data barang tersebut:\n",
        "\n",
        "Makanan (nama, harga, berat):\n",
        "1. Jajanan Ringan, Rp 10.000, 300g\n",
        "2. Nasi, Rp 5.000, 150g\n",
        "3. Ayam Goreng, Rp 10.000, 400g\n",
        "\n",
        "Minuman (nama, harga, volume):\n",
        "1. Aqua, Rp 3.000, 150ml\n",
        "2. Club, Rp 2.500, 50ml\n",
        "3. Es Teh, Rp 10.000, 250ml\n",
        "\n",
        "Pembeli ingin membeli makanan dan minuman dengan harga yang murah dengan berat yang paling besar. Jika pembeli memiliki uang sebesar 20.000,\n",
        "kira kira makanan dan minuman apa yang paling sesuai dengan keinginan pembeli? Makanan dan minuman hanya bisa dipilih sekali.\n",
        "'''\n",
        "list_makanan = [['Jajanan Ringan', 15000, 250], ['Nasi', 5000, 150], ['Ayam Goreng', 10000, 400]]\n",
        "list_minuman = [['Aqua', 3000, 150], ['Club', 2500, 150], ['Es Teh', 11000, 250]]\n",
        "uang = 20000\n",
        "\n",
        "total_harga = float('inf')\n",
        "total_berat = 0\n",
        "index_makanan = 0\n",
        "index_minuman = 0\n",
        "\n",
        "for x in range(len(list_makanan)):\n",
        "  for y in range(len(list_minuman)):\n",
        "    harga = list_makanan[x][1] + list_minuman[y][1]\n",
        "    berat = list_makanan[x][2] + list_minuman[y][2]\n",
        "\n",
        "    if harga <= uang:\n",
        "      print(f\"Makanan & Minuman : {list_makanan[x][0]} + {list_minuman[y][0]} = {harga}, {berat}g\")\n",
        "\n",
        "      if harga <= total_harga and berat >= total_berat:\n",
        "        total_harga = harga\n",
        "        total_berat = berat\n",
        "        index_makanan = x\n",
        "        index_minuman = y\n",
        "\n",
        "    else:\n",
        "      print(f\"Makanan & Minuman : {list_makanan[x][0]} + {list_minuman[y][0]} = {harga}, {berat}g - Terlalu Mahal\")\n",
        "\n",
        "print(f\"\\nMakanan paling sesuai dengan syarat pembeli adalah makanan {list_makanan[index_makanan][0]} dan minuman {list_minuman[index_minuman][0]}\")\n"
      ],
      "metadata": {
        "id": "BDW4QI3BOo0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üôä Sentiment Analysis"
      ],
      "metadata": {
        "id": "mu2V6xBxWEVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Dependendcy Library**"
      ],
      "metadata": {
        "id": "V9XSl21woKl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kaggle"
      ],
      "metadata": {
        "id": "n02w6i8XiQAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Utility Library\n",
        "import os\n",
        "from google.colab import files\n",
        "import csv\n",
        "import zipfile\n",
        "import re\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "# Tensorflow Library\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Data Preprocessing & Visualization Library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# NLTK Library\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "ZkIw2QKXi8HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Load Dataset**"
      ],
      "metadata": {
        "id": "N2nAlnK2hz60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(data_name):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a Kaggle competition dataset in Google Colab.\n",
        "\n",
        "    Args:\n",
        "        data_name (str): The name of the Kaggle competition dataset (e.g., \"learn-ai-bbc\").\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the extracted dataset folder.\n",
        "    \"\"\"\n",
        "    # Step 1: Upload kaggle.json\n",
        "    uploaded = {}\n",
        "    if not os.path.exists(\"/content/kaggle.json\"):\n",
        "      print(\"Please upload your kaggle.json file:\")\n",
        "      uploaded = files.upload()\n",
        "\n",
        "    if not os.path.exists(\"/content/kaggle.json\") and \"kaggle.json\" not in uploaded:\n",
        "        raise ValueError(\"kaggle.json file is required!\")\n",
        "\n",
        "    # Step 2: Move kaggle.json to the appropriate directory\n",
        "    kaggle_path = os.path.expanduser(\"~/.kaggle\")\n",
        "    os.makedirs(kaggle_path, exist_ok=True)\n",
        "    !mv kaggle.json {kaggle_path}/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    # Step 3: Download the dataset\n",
        "    print(f\"Downloading dataset: {data_name}\")\n",
        "    download_command = f\"kaggle competitions download -c {data_name}\"\n",
        "    download_status = os.system(download_command)\n",
        "\n",
        "    if download_status != 0:\n",
        "        raise RuntimeError(\"Failed to download the dataset. Ensure you've accepted the competition rules on Kaggle.\")\n",
        "\n",
        "    # Step 4: Extract the dataset\n",
        "    dataset_zip = f\"{data_name}.zip\"\n",
        "    dataset_folder = data_name + \"_dataset\"\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
        "            zip_ref.extractall(dataset_folder)\n",
        "        print(f\"Dataset successfully extracted to '{dataset_folder}'\")\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Zip file '{dataset_zip}' not found. Did the download complete?\")\n",
        "    except zipfile.BadZipFile:\n",
        "        raise ValueError(f\"Zip file '{dataset_zip}' is corrupted.\")\n",
        "\n",
        "    # Step 5: Return the path to the dataset folder\n",
        "    return dataset_folder"
      ],
      "metadata": {
        "id": "5KoMmRdMkyoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and Load Kaggle Dataset Name\n",
        "dataset_name = \"learn-ai-bbc\"\n",
        "load_dataset(dataset_name)"
      ],
      "metadata": {
        "id": "CgzHRMLpjtT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 1 : Data Understanding and Cleaning**"
      ],
      "metadata": {
        "id": "6-i8mrM2WOzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üîè: **Concept 1**"
      ],
      "metadata": {
        "id": "x0OoK9OTphrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning adalah bagian dari data pre-processing untuk mempersiapkan teks mentah agar lebih mudah untuk diproses.\n",
        "\n",
        "Dalam tahapan ini khususnya pada teks memiliki beberapa tujuan:\n",
        "1. Menghilangkan elemen tidak relevan.\n",
        "2. Melakukan standarisasi format teks.\n",
        "3. Mengurangi dimensi data dengan menghapus kata yang tidak penting.\n",
        "4. Mempermudah proses untuk analisis text."
      ],
      "metadata": {
        "id": "mwy4gd29p9ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Tahap Cleaning - Format Teks:\n",
        "\n",
        "- Tahapan ini menghapus karakter khusus seperti tanda baca, simbol, emoji.\n",
        "- Menghapus Angka yang tidak relevan\n",
        "- Menghapus URL yang terdapat dalam teks\n",
        "- Menghapus spasi berlebihan\n",
        "'''\n",
        "\n",
        "\n",
        "# Data Cleaning Function\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove special characters, punctuation, and emoji\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "data = [\n",
        "    \"I love programming!!! üòä\",\n",
        "    \"Python is THE best; language, isn't it?\",\n",
        "    \"Data science is the future in 2024!\",\n",
        "    \"Visit https://example.com for details.\",\n",
        "    \"   Machine   learning is FUN     \"\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"Text\"])\n",
        "\n",
        "# Apply the cleaning function to the DataFrame\n",
        "df[\"Cleaned\"] = df[\"Text\"].apply(clean_text)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original and Cleaned Text:\")\n",
        "df"
      ],
      "metadata": {
        "id": "y-3Ebj6xp6Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Tahap Cleaning - Lowercase :\n",
        "\n",
        "- Melakukan format teks ini seperti mengencilkan semua huruf\n",
        "  membuat data semakin tidak kompleks karena perbedaan huruf besar dan kecil.\n",
        "'''\n",
        "\n",
        "df['lower'] = df['Cleaned'].str.lower()\n",
        "df"
      ],
      "metadata": {
        "id": "tualJUo2s2PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Tahap Cleaning - Tokenization dan Stopword Removal:\n",
        "\n",
        "- Tahap Tokenisasi ini akan memecah kalimat menjadi kata atau token sehingga\n",
        "  lebih mudah untuk melakukan pemrosesan dan pemaknaan.\n",
        "\n",
        "- Tahap stopword adalah tahap untuk menghilangkan kata yang tidak memiliki\n",
        "  makna signifikan seperti kata hubung (dan, yang, atau)\n",
        "'''\n",
        "\n",
        "# Melakukan Tokenisasi\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "text_tokenize = df['lower'].apply(tokenize_text)\n",
        "df['tokenize'] = text_tokenize\n",
        "\n",
        "# Melakukan Stopword Removal\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "text_without_stopwords = df['tokenize'].apply(remove_stopwords)\n",
        "df['remove_stopwords'] = text_without_stopwords\n",
        "df"
      ],
      "metadata": {
        "id": "Pu4ZB7NjuTZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Tahap Stemming atau Lemmatization\n",
        "- Stemming lebih agresif dan cepat, tetapi terkadang menghasilkan kata yang tidak valid.\n",
        "  Memotong kata ke bentuk dasar, tetapi seringkali menghasilkan kata yang tidak selalu merupakan\n",
        "  bentuk yang sah dalam bahasa. Misalnya, kata \"running\" bisa dipotong menjadi \"run\",\n",
        "  tetapi kata \"better\" mungkin menjadi \"bet\".\n",
        "\n",
        "- Lemmatization lebih tepat dan mempertahankan makna, tetapi lebih kompleks karena\n",
        "  melibatkan analisis konteks. Mengurangi kata ke bentuk dasar yang benar-benar ada dalam bahasa dan sesuai dengan konteksnya.\n",
        "  Misalnya, \"running\" akan menjadi \"run\" dan \"better\" akan menjadi \"good\".\n",
        "'''\n",
        "\n",
        "# Melakukan Steaming\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(tokens):\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "df['stemmed'] = df['remove_stopwords'].apply(stem_words)\n",
        "\n",
        "# Melakukan Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_words(tokens):\n",
        "    return [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
        "\n",
        "df['lemmatized'] = df['remove_stopwords'].apply(lemmatize_words)\n",
        "\n",
        "# Tampilkan Data\n",
        "df"
      ],
      "metadata": {
        "id": "gp_z3Phouxr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üìù: **Implementation**"
      ],
      "metadata": {
        "id": "M6Kpc91_phzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fungsi Parsing Data**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Pt8ZY_Yg6Jqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sentence_labels(filename):\n",
        "    \"\"\"\n",
        "    Parses a CSV file containing sentences and their corresponding labels.\n",
        "\n",
        "    Args:\n",
        "    filename (str): The path to the CSV file to parse.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing two lists:\n",
        "        - sentences (list of str): A list of sentences from the CSV file.\n",
        "        - labels (list of str): A list of labels corresponding to the sentences.\n",
        "\n",
        "    Raises:\n",
        "    FileNotFoundError: If the specified file does not exist.\n",
        "    ValueError: If the CSV file has an unexpected structure or empty rows.\n",
        "    \"\"\"\n",
        "    # Ensure the file exists before attempting to read it\n",
        "    if not os.path.exists(filename):\n",
        "        raise FileNotFoundError(f\"The file at {filename} does not exist.\")\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as csvfile:\n",
        "            reader = csv.reader(csvfile, delimiter=',')\n",
        "\n",
        "            # Skip header row if it exists\n",
        "            next(reader)\n",
        "\n",
        "            for row in reader:\n",
        "                # Check if row has the expected number of columns\n",
        "                if len(row) < 3:\n",
        "                    raise ValueError(f\"Row is missing required columns: {row}\")\n",
        "\n",
        "                # Append the label (index 2) and sentence (index 1) from each row\n",
        "                labels.append(row[2])\n",
        "                sentences.append(row[1])\n",
        "\n",
        "    except csv.Error as e:\n",
        "        raise ValueError(f\"Error reading the CSV file: {e}\")\n",
        "\n",
        "    return sentences, labels"
      ],
      "metadata": {
        "id": "zcDI1Ie13ggX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fungsi Data Cleaning**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S4Mx4PNF6U0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_cleaning(sentence):\n",
        "    \"\"\"\n",
        "    This function cleans and preprocesses a single sentence by performing several operations:\n",
        "    1. Lowercasing\n",
        "    2. Removing URLs\n",
        "    3. Removing special characters, punctuation, and emojis\n",
        "    4. Removing numbers\n",
        "    5. Removing extra whitespaces\n",
        "    6. Removing stopwords\n",
        "\n",
        "    Args:\n",
        "    sentence (str): The sentence to be cleaned\n",
        "\n",
        "    Returns:\n",
        "    str: The cleaned sentence\n",
        "    \"\"\"\n",
        "\n",
        "    # List of stopwords\n",
        "\n",
        "    # Using Pre-Define Stopword\n",
        "    #stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "    # Using NLTK Stopword\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "    # Lowered Sentence\n",
        "    text = sentence.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', sentence)\n",
        "\n",
        "    # Remove special characters, punctuation, and emoji\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove Stopword\n",
        "    words = sentence.split()\n",
        "    no_stopwords = [word for word in words if word not in stopwords]\n",
        "    cleaned_text = \" \".join(no_stopwords)\n",
        "\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "Af2a-dhS6UVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_lemmatization(lemmatizer, sentence):\n",
        "    \"\"\"\n",
        "    This function lemmatizes a list of words in the given sentence.\n",
        "    It lemmatizes each word with the part of speech as verb ('v') and then recombines the lemmatized words into a sentence.\n",
        "\n",
        "    Args:\n",
        "    lemmatizer (WordNetLemmatizer): The lemmatizer to use.\n",
        "    sentence (list of str): The list of words in the sentence.\n",
        "\n",
        "    Returns:\n",
        "    str: A recombined sentence after lemmatization\n",
        "    \"\"\"\n",
        "    # Lemmatizing each word\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in sentence]\n",
        "\n",
        "    # Combining the lemmatized words back into a sentence\n",
        "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
        "\n",
        "    return lemmatized_sentence\n"
      ],
      "metadata": {
        "id": "hToUBtHxBSVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proses Data Cleaning**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Xsz7mgiUGMsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Data Train Parse Sentence and Label\n",
        "sentences, labels = parse_sentence_labels(\"/content/learn-ai-bbc_dataset/BBC News Train.csv\")\n",
        "\n",
        "# Check Morfologies Sentence and Labels\n",
        "check_index=0\n",
        "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
        "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
        "print(f\"The first 5 sentences are :\\n{sentences[check_index]}\\n\")\n",
        "print(f\"The first 5 labels are : \\n{labels[check_index]}\\n\")\n",
        "print(f\"The unique labels : \\n{set(labels)}\")"
      ],
      "metadata": {
        "id": "TQ66Cnv1WN4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tahapan Data Cleaning - Lowering Text, Format Teks and Remove Stopwords\n",
        "cleaned_sentences = [data_cleaning(sentence) for sentence in sentences]\n",
        "\n",
        "# Check Result\n",
        "check_index=0\n",
        "print(f\"The first sentences are before data cleaning:\\n{sentences[check_index]}\\n\")\n",
        "print(f\"The first sentences are after data cleaning:\\n{cleaned_sentences[check_index]}\\n\")"
      ],
      "metadata": {
        "id": "guj2L_L16xje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tahapan Data Cleaning - Lemmatization\n",
        "\n",
        "# Define Lemmatizer Engine\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenize first all sentences\n",
        "tokenized_sentences = [sentence.split() for sentence in cleaned_sentences]\n",
        "\n",
        "# Lemmatization all sentences\n",
        "lemmatized_sentences = [data_lemmatization(lemmatizer, sentence) for sentence in tokenized_sentences]\n",
        "\n",
        "# Check Result\n",
        "check_index=0\n",
        "print(f\"The first sentences are before data lemmatization:\\n{cleaned_sentences[check_index]}\\n\")\n",
        "print(f\"The first sentences are after data lemmatization:\\n{lemmatized_sentences[check_index]}\\n\")"
      ],
      "metadata": {
        "id": "IYAb097aANX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 2 : Tokenizing, Padding, Sequence**"
      ],
      "metadata": {
        "id": "RMMrieKYhoks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üîè: **Concept 2**"
      ],
      "metadata": {
        "id": "yMynh2Rgo5vL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tahap Tokenizing, Padding, dan Sequence adalah tahap untuk merubah sebuah string atau text kedalam sebuah bilangan integer yang bisa dimengerti oleh mesin.\n",
        "\n",
        "Dalam proses perubahan ini akan ada beberapa tahapan yang penting:\n",
        "- 1Ô∏è‚É£ **Tahap Tokenizing**\n",
        "  \n",
        "  Tahap tokenizing adalah tahap untuk memecah kalimat yang besar menjadi kumpulan kata/token yang mewakili hanya 1 kata. Pada tahap ini juga akan didefinisikan sebuah variabel kamus katayang berisi kode dari kata yang dilakukan prubahan.\n",
        "\n",
        "  Kode kata biasanya akan dalam bentuk dictionary dengan nilai integer sebagai perwakilannya.\n",
        "\n",
        "- 2Ô∏è‚É£ **Tahap Sequence**\n",
        "\n",
        "  Tahap sequence adalah tahap lanjutan dari tokenizing. Setelah berhasil dalam membuat kamus data maka selanjutnya kita bisa memanfaatkan kamus data tersebut untuk menyusun kalimat berdasarkan dengan kamus data.\n",
        "\n",
        "- 3Ô∏è‚É£ **Tahap Padding**\n",
        "\n",
        "  Tahap padding adalah tahap lanjutan dari sequence, setelah berhasil membuat sequence of the text, maka akan terdapat perbedaan panjang text antara satu sequence dengan yang lainnya, oleh karena itu padding berfungsi untuk menseragamkan text.\n"
      ],
      "metadata": {
        "id": "u_GyC0MuERVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Tahap Tokenizing\n",
        "\n",
        "Tahap ini akan memecah kata dan membuat sebuah kamus kata yang berisi kode dari kata yang dilakukan prubahan.\n",
        "Kode kata biasanya akan dalam bentuk dictionary dengan nilai integer sebagai perwakilannya.\n",
        "\n",
        "Sebagai contoh:\n",
        "\n",
        "  kamus_kata = {aku: 1, suka: 2, makan: 3, roti: 4}\n",
        "\n",
        "Representasi kamusnya:\n",
        "\n",
        "  kata = [1,2,3,4]\n",
        "'''\n",
        "\n",
        "# Define the Sentence Example\n",
        "sentences = [\n",
        "    \"I love programming ü§£\",\n",
        "    \"Python is the best language\",\n",
        "    \"Data science is the future!!\"\n",
        "]\n",
        "\n",
        "# Define the Engine for Tokenizer\n",
        "tokenizer = Tokenizer(num_words=10, oov_token=\"<OOV>\")\n",
        "\n",
        "# Generate corpus for each texts\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the index of word vocab\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Check Result\n",
        "print(word_index)"
      ],
      "metadata": {
        "id": "qpfqRK8TpaPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Tahap Sequence\n",
        "\n",
        "Tahap sequence adalah tahap lanjutan dari tokenizing.\n",
        "Setelah berhasil dalam membuat kamus data maka selanjutnya kita bisa memanfaatkan kamus data\n",
        "tersebut untuk menyusun kalimat berdasarkan dengan kamus data.\n",
        "'''\n",
        "\n",
        "# Define the sequence variabel and generate list of token sequence\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Check Result\n",
        "print(sequences)"
      ],
      "metadata": {
        "id": "6TEM3tHqHV31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Tahap Padding\n",
        "\n",
        "Tahap padding akan melakukan penseragaman sequence of text sehingg\n",
        "text nantinya bisa memiliki panjang data yang sama antara satu dengan lainnya.\n",
        "'''\n",
        "\n",
        "# Define variabel for maximum length\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "\n",
        "# Define padded sequence variabel\n",
        "padded_sequence_1 = pad_sequences(sequences, maxlen=3)\n",
        "padded_sequence_2 = pad_sequences(sequences, maxlen=6, padding='post')\n",
        "padded_sequence_3 = pad_sequences(sequences, maxlen=6, truncating='post')\n",
        "padded_sequence_4 = pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n",
        "\n",
        "# Check Result\n",
        "print(padded_sequence_1)\n",
        "print(\"\\n\", padded_sequence_2)\n",
        "print(\"\\n\", padded_sequence_3)\n",
        "print(\"\\n\", padded_sequence_4)"
      ],
      "metadata": {
        "id": "xm_o9KR9Inj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Out-of-Vocabulary Tokens\n",
        "\n",
        "Konsep ini adalah konsep tambahan dengan asumsi bagaimana jika\n",
        "sebauh data atau kalimat ada kata-katanya yang tidak ada dalam kamus data?\n",
        "\n",
        "Untuk itu kita mengatakannya sebagai (OOV) atau Out of Vocabulary.\n",
        "'''\n",
        "\n",
        "# Define the test sentence\n",
        "test_sentences = ['Python is the turtle language']\n",
        "\n",
        "# Define and generate the test sequence\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "# Check the result\n",
        "print(word_index)\n",
        "print(\"\\n\",test_sequences)"
      ],
      "metadata": {
        "id": "xZA4dcHPMZGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üìù: **Implementation**"
      ],
      "metadata": {
        "id": "XiiSQAKrpJga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fungsi Tokenizing, Padding, and Sequence**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JVarVqT8RJmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizing_padding_sequence(sequence, num_word=None, oov_token=\"<OOV>\", max_len=\"dynamic\", padding=\"post\", truncating=\"post\"):\n",
        "    \"\"\"\n",
        "    Tokenizes and pads a list of text sequences for neural network input.\n",
        "\n",
        "    Parameters:\n",
        "    - sequence (list of str): The input text sequences to tokenize and pad.\n",
        "    - num_word (int, optional): The maximum number of words to keep in the vocabulary based on frequency.\n",
        "    - oov_token (str, optional): Token to represent out-of-vocabulary words. Default is \"<OOV>\".\n",
        "    - max_len (int or \"dynamic\", optional): Maximum length of the padded sequences.\n",
        "      If set to \"dynamic\", it uses the length of the longest sequence. Default is \"dynamic\".\n",
        "    - padding (str, optional): Padding strategy (\"pre\" or \"post\"). Default is \"post\".\n",
        "    - truncating (str, optional): Truncation strategy (\"pre\" or \"post\"). Default is \"post\".\n",
        "\n",
        "    Returns:\n",
        "    - word_index (dict): A dictionary mapping words to their respective indices.\n",
        "    - sequences (list of list of int): List of integer sequences corresponding to input texts.\n",
        "    - padded_sequence (numpy.ndarray): Padded sequences as a NumPy array.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Define and generate tokenizer and word_index\n",
        "    tokenizer = Tokenizer(num_words=num_word, oov_token=oov_token)\n",
        "    tokenizer.fit_on_texts(sequence)\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    # Step 2: Generate the sequences\n",
        "    sequences = tokenizer.texts_to_sequences(sequence)\n",
        "\n",
        "    # Step 3: Generate the padded sequences\n",
        "    if max_len == \"dynamic\":\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "\n",
        "    padded_sequence = pad_sequences(sequences, maxlen=max_len, padding=padding, truncating=truncating)\n",
        "\n",
        "    return word_index, sequences, padded_sequence"
      ],
      "metadata": {
        "id": "N3doS7cJhoQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_labels(labels):\n",
        "    \"\"\"\n",
        "    Tokenizes a list of labels into numerical sequences.\n",
        "\n",
        "    Args:\n",
        "        labels (List[str]): A list of category labels.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, int], np.ndarray, Tokenizer]:\n",
        "            - `label_word_index`: Dictionary mapping labels to their corresponding token indices.\n",
        "            - `label_sequences`: NumPy array of tokenized label sequences (zero-indexed).\n",
        "            - `label_tokenizer`: The fitted Keras Tokenizer object (useful for saving/loading).\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Make the tokenizer engine\n",
        "    label_tokenizer = Tokenizer()\n",
        "\n",
        "    # Step 2: Fit the tokenizer texts\n",
        "    label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "    # Step 3: This is optional, get the index of the tokenize labels\n",
        "    label_word_index = label_tokenizer.word_index\n",
        "\n",
        "    # Step 4: Save the sequences label\n",
        "    label_sequences = label_tokenizer.texts_to_sequences(labels)\n",
        "\n",
        "    return label_word_index, label_sequences"
      ],
      "metadata": {
        "id": "YUAMzbnvNGqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proses Data Transformation**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bWVGmp4YRWrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the data transformation\n",
        "word_index, sequence, padded_sequence = tokenizing_padding_sequence(lemmatized_sentences, oov_token=\"<OOV>\", max_len=\"dynamic\", padding=\"post\", truncating=\"post\")\n",
        "\n",
        "# Check the result\n",
        "print(word_index)\n",
        "print(f\"Example Sentences:\\n{sequence[0]}\")\n",
        "print(f\"\\nExample Padded Sequence:\\n{padded_sequence[0]}\")"
      ],
      "metadata": {
        "id": "HRBbfOLTUtGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the labels transformation\n",
        "label_word_index, label_sequences = tokenize_labels(labels)\n",
        "\n",
        "# Check the result\n",
        "print(label_word_index)\n",
        "print(f\"Example Labels:\\n{label_sequences[0]}\")"
      ],
      "metadata": {
        "id": "mBoH-9C5MyBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3 : Data Splitting and Modeling**"
      ],
      "metadata": {
        "id": "XRDZrLHGZ_tF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üîè: **Concept 3**"
      ],
      "metadata": {
        "id": "k3Ln5Z6laG1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahapan kali ini adalah konsep dalam melakukan data splitting. Data splitting adalah proses yang krusial dalam pembangunan sebuah model yang mana akan menentukan jumlah data yang di trainning ataupun di testing sedhingga menciptakan model dengan performa yang efektif.\n",
        "\n",
        "Dalam data splitting terhadap beberapa metode yang bisa dilakukan seperti:\n",
        "1. Metode Holdout\n",
        "2. Metode K-Fold Cross Validation\n",
        "3. Metode Stratified Splitting\n",
        "\n",
        "Untuk metode kali ini akan mencoba metode holdout dengan melakukan splitting secara random, sehingga proses splitting cepat dan mudah diimplementasikan."
      ],
      "metadata": {
        "id": "MOD2L9ykTzmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Metode Holdout\n",
        "\n",
        "Metode ini akan membagi data yang sudah dibuat dengan melakukan pembagian sesuai dengan rasio yang ditentukan.\n",
        "Metode ini sangat sederhana dan mudah untuk implementasi sehingga sangat cocok untuk sistemyang memerlukan memori sedikit.\n",
        "'''\n",
        "\n",
        "# Define the data and the labels\n",
        "data = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12],\n",
        "    [13, 14, 15],\n",
        "    [16, 17, 18],\n",
        "    [19, 20, 21],\n",
        "    [22, 23, 24],\n",
        "    [25, 26, 27],\n",
        "    [28, 29, 30]\n",
        "])\n",
        "\n",
        "con_labels = np.array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
        "\n",
        "# Define the split rasio and make an split index\n",
        "split_ratio = 0.9\n",
        "split_index= int(len(data)*split_ratio)\n",
        "\n",
        "# Splitting the data using the ratio and index\n",
        "train_data, test_data= data[:split_index], data[split_index:]\n",
        "train_labels, test_labels= con_labels[:split_index], con_labels[split_index:]\n",
        "\n",
        "# Check the result\n",
        "print(\"Training Data:\\n\", train_data)\n",
        "print(\"\\nTraining Labels:\\n\", train_labels)\n",
        "print(\"\\nTesting Data:\\n\", test_data)\n",
        "print(\"\\nTesting Labels:\\n\", test_labels)"
      ],
      "metadata": {
        "id": "tns6D4XFaLnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tahap selanjutnya adalah data modelling. Pada tahap ini akan dilakukan pemilihan model machine learning ataupun model deeplearning yang akan digunakan untuk memproses model dalam melakukan klasifikasi terhadap data."
      ],
      "metadata": {
        "id": "FZednfBDg_1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üìù: **Implementation**"
      ],
      "metadata": {
        "id": "ESZrpM5-aI9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fungsi Splitting Data**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YLYNEI2MbMo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(sentences, labels, split_ratio):\n",
        "  \"\"\"\n",
        "  Splits the given dataset into training and testing sets based on the specified ratio.\n",
        "\n",
        "  Args:\n",
        "      sentences (list or np.array): The input data (e.g., text sentences or feature vectors).\n",
        "      labels (list or np.array): The corresponding labels for the data.\n",
        "      split_ratio (float): The proportion of data to be used for training (e.g., 0.8 for 80% train, 20% test).\n",
        "\n",
        "  Returns:\n",
        "      tuple: (train_sentences, train_labels, test_sentences, test_labels)\n",
        "          - train_sentences: Training portion of the input data.\n",
        "          - train_labels: Training portion of the labels.\n",
        "          - test_sentences: Testing portion of the input data.\n",
        "          - test_labels: Testing portion of the labels.\n",
        "  \"\"\"\n",
        "  size = int(len(sentences) * split_ratio)\n",
        "\n",
        "  train_sentences, test_sentences = sentences[:size], sentences[size:]\n",
        "  train_labels, test_labels = labels[:size], labels[size:]\n",
        "\n",
        "  return train_sentences, train_labels, test_sentences, test_labels"
      ],
      "metadata": {
        "id": "JmAqpfv0aMFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proses Data Splitting**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fPrhVvJudr1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the data split using function that have been built\n",
        "train_data, train_labels, test_data, test_labels = split_data(lemmatized_sentences, labels, 0.9)\n",
        "\n",
        "# Check the result\n",
        "print(\"Training Data:\\n\", train_data[0])\n",
        "print(\"\\nTraining Labels:\\n\", train_labels[0])\n",
        "print(\"\\nTesting Data:\\n\", test_data[0])\n",
        "print(\"\\nTesting Labels:\\n\", test_labels[0])"
      ],
      "metadata": {
        "id": "jyzS1EQvdoxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of the labels\n",
        "\n",
        "train_label_count = Counter(train_labels)\n",
        "test_label_count = Counter(test_labels)\n",
        "\n",
        "print(\"Trainning Label Counts\")\n",
        "for label, count in train_label_count.items():\n",
        "    print(f\"Label {label}: {count}\")\n",
        "\n",
        "print(\"\\nTesting Label Counts\")\n",
        "for label, count in test_label_count.items():\n",
        "    print(f\"Label {label}: {count}\")"
      ],
      "metadata": {
        "id": "0S3m4aAqfZ43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proses Pra-modeling (Tokenizer, Padding, and Sequence)**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lDEOAVB87PHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_tokenizer(sentences, num_words, oov_token=\"<OOV>\"):\n",
        "    \"\"\"\n",
        "    Initializes and fits a Keras Tokenizer on a given list of sentences.\n",
        "\n",
        "    Args:\n",
        "        sentences (list of str): List of sentences to train the tokenizer.\n",
        "        num_words (int): Maximum number of words to keep, based on word frequency.\n",
        "        oov_token (str, optional): Token to represent out-of-vocabulary words. Default is \"<OOV>\".\n",
        "\n",
        "    Returns:\n",
        "        Tokenizer: A fitted Keras Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "IewDwaAR7TNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_padding(sentences, tokenizer, padding, maxlen):\n",
        "    \"\"\"\n",
        "    Converts text sentences into padded sequences.\n",
        "\n",
        "    Args:\n",
        "        sentences (list of str): List of sentences to convert into sequences.\n",
        "        tokenizer (Tokenizer): A trained Keras Tokenizer instance.\n",
        "        maxlen (int): Maximum sequence length after padding.\n",
        "        padding (str, optional): Padding type - 'pre' or 'post'. Default is 'post'.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Padded sequence representation of the input sentences.\n",
        "    \"\"\"\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    padded_sequence = pad_sequences(sequences, maxlen=maxlen, padding=padding)\n",
        "    return padded_sequence"
      ],
      "metadata": {
        "id": "5txuM0Uc9Wnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer_seq_pad_label(tokenizer, split_labels):\n",
        "    \"\"\"\n",
        "    Tokenizes labels and converts them into a numerical sequence.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (Tokenizer): The fitted tokenizer for labels.\n",
        "        split_labels (list of str): List of labels to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Numpy array of tokenized labels, zero-indexed.\n",
        "    \"\"\"\n",
        "\n",
        "    label_sequences = tokenizer.texts_to_sequences(split_labels)\n",
        "    np_labels = np.array(label_sequences, dtype=np.int32)\n",
        "\n",
        "    if np_labels.size == 0:\n",
        "        raise ValueError(\"No valid labels found in split_labels.\")\n",
        "\n",
        "    return np_labels - 1"
      ],
      "metadata": {
        "id": "RpRFZLruAdUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variabel for tokenization\n",
        "num_word = 21032\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "\n",
        "# Make train sentence tokenizer\n",
        "tokenizer = fit_tokenizer(train_data, num_word, OOV_TOKEN)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Check the result\n",
        "print(f\"Total vocabulary train sentences is {len(word_index)} words\")"
      ],
      "metadata": {
        "id": "fjYPrZ-Z99Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variabel for padding and sequence\n",
        "PADDING = 'post'\n",
        "MAXLEN = 150\n",
        "\n",
        "# Do the padding and sequencing using premade tokenizer\n",
        "train_padded_seq = sequence_padding(train_data, tokenizer, PADDING, MAXLEN)\n",
        "test_padded_seq = sequence_padding(test_data, tokenizer, PADDING, MAXLEN)\n",
        "\n",
        "# Check the result\n",
        "print(f\"Shape of training padded sequence is {train_padded_seq.shape}\")\n",
        "print(f\"Shape of testing padded sequence is {test_padded_seq.shape}\")"
      ],
      "metadata": {
        "id": "xIUJNtPP_p0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenizer\n",
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "# Tokenize, sequence, and padding the label\n",
        "train_label_seq = tokenizer_seq_pad_label(label_tokenizer, train_labels)\n",
        "test_label_seq = tokenizer_seq_pad_label(label_tokenizer, test_labels)\n",
        "\n",
        "# Check the result\n",
        "print(f\"Example for 5 train labels:\\n{train_label_seq[0:5]}\")\n",
        "print(f\"Example for 5 test labels:\\n{test_label_seq[0:5]}\")\n",
        "print(f\"Shape of training label sequence is {train_label_seq.shape}\")\n",
        "print(f\"Shape of testing label sequence is {test_label_seq.shape}\")"
      ],
      "metadata": {
        "id": "8_Z2OPkzBLLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proses Modeling**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Qd3avxKxCetR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_words, embedding_dim, maxlen, num_classes=5):\n",
        "    \"\"\"\n",
        "    Creates and compiles a text classification model using an embedding layer.\n",
        "\n",
        "    Args:\n",
        "        num_words (int): Vocabulary size for the embedding layer.\n",
        "        embedding_dim (int): Dimensionality of the embedding vectors.\n",
        "        maxlen (int): Maximum length of input sequences.\n",
        "        num_classes (int): Number of output classes.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: A compiled Keras model.\n",
        "    \"\"\"\n",
        "    tf.random.set_seed(199)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(num_words, embedding_dim),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.build(input_shape=(None, maxlen))\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "CRYG9y2jDLtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variabel for model machine learning\n",
        "EMBEDDING_DIM = 8\n",
        "\n",
        "# Create model machine learning\n",
        "model = create_model(num_word, EMBEDDING_DIM, MAXLEN, num_classes=len(set(labels)))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Ol3nsTSDD6q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Layers (Optional)**\n",
        "\n",
        "---\n",
        "\n",
        "use this code to see the embedding layers implementation in words"
      ],
      "metadata": {
        "id": "S4x7U2dDF3up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse word index\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# Get the first layers\n",
        "embedding_layer = model.layers[0]\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "print(embedding_weights.shape)"
      ],
      "metadata": {
        "id": "5xBXcgAxF_FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate files for embedding visualization\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, num_word):\n",
        "    word = reverse_word_index[word_num]\n",
        "    embeddings = embedding_weights[word_num]\n",
        "    out_m.write(word + \"\\n\")\n",
        "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "metadata": {
        "id": "R8NZMK6yJphq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trainning Model**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ZMYcvYPFNfBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_padded_seq,\n",
        "    train_label_seq,\n",
        "    epochs=25,\n",
        "    validation_data=(test_padded_seq, test_label_seq)\n",
        ")"
      ],
      "metadata": {
        "id": "tgDzckQUNjn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history[f'val_{metric}'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, f'val_{metric}'])\n",
        "    plt.show()\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "id": "aEgf33fCN6eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Model**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GmQjiS0ngBSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "test_df = pd.read_csv(\"/content/learn-ai-bbc_dataset/BBC News Test.csv\")\n",
        "test_sentences = test_df['Text'].tolist()\n",
        "\n",
        "# Apply same cleaning function\n",
        "cleaned_test_sentences = [data_cleaning(sentence) for sentence in test_sentences]\n",
        "\n",
        "check_index=0\n",
        "print(\"== CLEANING RESULT ==\")\n",
        "print(f\"The first sentences before data cleaning:\\n{test_sentences[check_index]}\\n\")\n",
        "print(f\"The first sentences after data cleaning:\\n{cleaned_test_sentences[check_index]}\\n\")\n",
        "\n",
        "# Apply same lemmatization function\n",
        "tokenized_test_sentences = [sentence.split() for sentence in cleaned_test_sentences]\n",
        "lemmatized_test_sentences = [data_lemmatization(lemmatizer, sentence) for sentence in tokenized_test_sentences]\n",
        "print(\"== LEMMATIZATION RESULT ==\")\n",
        "print(f\"The first sentences before data lemmatization:\\n{cleaned_test_sentences[check_index]}\\n\")\n",
        "print(f\"The first sentences after data lemmatization:\\n{lemmatized_test_sentences[check_index]}\\n\")"
      ],
      "metadata": {
        "id": "YF2bfMbogFUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply same tokenizer, sequence, and padding to test data\n",
        "test_final = sequence_padding(lemmatized_test_sentences, tokenizer, PADDING, MAXLEN)\n",
        "print(f\"Shape of testing padded sequence is {test_final.shape}\")"
      ],
      "metadata": {
        "id": "9R2KzZ58i9I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do prediction model\n",
        "predictions=model.predict(test_final)"
      ],
      "metadata": {
        "id": "HPQKkJr1kdob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get maximum indices indicate the class\n",
        "predicted_indices = np.argmax(predictions, axis=1)\n",
        "index_to_label = {v - 1: k for k, v in label_tokenizer.word_index.items()}\n",
        "predicted_labels = [index_to_label[idx] for idx in predicted_indices]\n",
        "test_df[\"Predicted_Label\"] = predicted_labels"
      ],
      "metadata": {
        "id": "aj6rG9y8mD0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the result\n",
        "test_df"
      ],
      "metadata": {
        "id": "zUSflDv2rjXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To compare the result test with solution, load the solution in dataframe\n",
        "solution_df = pd.read_csv(\"/content/learn-ai-bbc_dataset/BBC News Sample Solution.csv\")\n",
        "\n",
        "# Merge test predictions with the solution based on \"ArticleId\"\n",
        "merged_df = test_df[[\"ArticleId\", \"Predicted_Label\"]].merge(\n",
        "    solution_df, on=\"ArticleId\", how=\"inner\"\n",
        ")\n",
        "\n",
        "# Ensure alignment by sorting\n",
        "merged_df = merged_df.sort_values(by=\"ArticleId\").reset_index(drop=True)\n",
        "\n",
        "# Check the result\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "ATf_ozZFF9xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the correct labels are in the 'Label' column of solution_df\n",
        "print(classification_report(merged_df[\"Category\"], merged_df[\"Predicted_Label\"]))"
      ],
      "metadata": {
        "id": "CNmg_2K8HTyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 4 : Implementation Webapp**"
      ],
      "metadata": {
        "id": "COmMWln1hKUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üîÅ **Set-up Ngrok and Environment for Streamlit**"
      ],
      "metadata": {
        "id": "iSUQEV-eusV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ngrok library\n",
        "!pip install -q streamlit pyngrok"
      ],
      "metadata": {
        "id": "bvTIJtAjt9D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authentification with ngrok token\n",
        "!ngrok authtoken ngrok_token"
      ],
      "metadata": {
        "id": "cU7PDR8RuBg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model machine learning\n",
        "model.save('news_classification_model.keras')"
      ],
      "metadata": {
        "id": "iYWkXf5Eu9sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tokenizer\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "with open('label_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(label_tokenizer, f)"
      ],
      "metadata": {
        "id": "D6yASym5wPL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üçé **Streamlit App**"
      ],
      "metadata": {
        "id": "amegxxi6u36z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dashboard.py\n",
        "\n",
        "# Import Library\n",
        "import re\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "def data_cleaning(sentence):\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    text = sentence.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', sentence)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    words = sentence.split()\n",
        "    no_stopwords = [word for word in words if word not in stopwords]\n",
        "    cleaned_text = \" \".join(no_stopwords)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def data_lemmatization(lemmatizer, sentence):\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in sentence]\n",
        "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
        "\n",
        "    return lemmatized_sentence\n",
        "\n",
        "def preprocess_input(title, tokenizer, maxlen, padding):\n",
        "    title_seq = tokenizer.texts_to_sequences([title])\n",
        "    title_padded = pad_sequences(title_seq, maxlen=maxlen, padding=padding)\n",
        "    return title_padded\n",
        "\n",
        "def get_label(predictions, tokenizer):\n",
        "    predicted_indices = np.argmax(predictions, axis=1)\n",
        "    index_to_label = {v - 1: k for k, v in label_tokenizer.word_index.items()}\n",
        "    predicted_labels = [index_to_label[idx] for idx in predicted_indices]\n",
        "    return predicted_labels\n",
        "\n",
        "# Load the model from .keras format and variabel\n",
        "model = tf.keras.models.load_model('news_classification_model.keras')\n",
        "PADDING = 'post'\n",
        "MAXLEN = 150\n",
        "\n",
        "# Load tokenizer\n",
        "with open('tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "with open('label_tokenizer.pkl', 'rb') as f:\n",
        "    label_tokenizer = pickle.load(f)\n",
        "\n",
        "# Set up the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Main Process\n",
        "# Streamlit app layout\n",
        "st.title(\"Title Category Prediction\")\n",
        "st.write(\"Enter a news article title, and we'll predict what category it's about.\")\n",
        "\n",
        "# User input\n",
        "title_input = st.text_input(\"Enter title:\")\n",
        "\n",
        "# Preprocessing text-input\n",
        "if title_input:\n",
        "    # Main process - preprocessing input\n",
        "    cleaned_text = data_cleaning(title_input)\n",
        "    tokenized_text = word_tokenize(cleaned_text)\n",
        "    lemmatized_text = data_lemmatization(lemmatizer, tokenized_text)\n",
        "\n",
        "    # Main process - make predictions and get labels\n",
        "    preprocessed_input = preprocess_input(lemmatized_text, tokenizer, MAXLEN, PADDING)\n",
        "    predictions=model.predict(preprocessed_input)\n",
        "    labels = get_label(predictions, label_tokenizer)\n",
        "\n",
        "    # Main process - show the output\n",
        "    st.write(\"Predicted Category:\", labels[0])"
      ],
      "metadata": {
        "id": "0GY9J8tRhQwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üèÉüèª‚Äç‚ôÄÔ∏è **Running App with Ngrok Tunnel**"
      ],
      "metadata": {
        "id": "wqDCVyKP8wbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_streamlit_app(app_name, port=8501):\n",
        "    \"\"\"\n",
        "    Runs a Streamlit app and sets up an ngrok tunnel to expose it to the internet.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    app_name : str\n",
        "        The name of the Streamlit app file (e.g., \"app.py\").\n",
        "    port : int, optional, default=8501\n",
        "        The port on which to run the Streamlit app (default is 8501).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        The public URL provided by ngrok for the Streamlit app.\n",
        "    \"\"\"\n",
        "    # Kill any previous tunnels\n",
        "    ngrok.kill()\n",
        "\n",
        "    # Run the Streamlit app\n",
        "    subprocess.Popen([\"streamlit\", \"run\", app_name, \"--server.port\", str(port)])\n",
        "\n",
        "    # Start a new ngrok tunnel\n",
        "    ngrok_tunnel = ngrok.connect(port, \"http\")\n",
        "\n",
        "    # Print the public URL for the Streamlit app\n",
        "    print(\"Streamlit URL:\", ngrok_tunnel.public_url)\n",
        "\n",
        "    return ngrok_tunnel.public_url"
      ],
      "metadata": {
        "id": "LNiofYWi82WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "app_name = \"dashboard.py\"\n",
        "run_streamlit_app(app_name)"
      ],
      "metadata": {
        "id": "xcREEcEa9GmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Shutdown Server**\n"
      ],
      "metadata": {
        "id": "DiIB3go7scsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep ngrok"
      ],
      "metadata": {
        "id": "YCtcolxasknn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "riA0rapjspFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep ngrok"
      ],
      "metadata": {
        "id": "8ghe9uM-sqsf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}